{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9ad8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input data to be non-empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39mloss)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Entrenamiento\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 5. Función de generación\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(model, start_string, num_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\vpn\\Documents\\GitHub\\Ciencia-de-datos\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vpn\\Documents\\GitHub\\Ciencia-de-datos\\.venv\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1319\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_dataset_and_inferred_steps(\n\u001b[0;32m   1315\u001b[0m     strategy, x, steps_per_epoch, class_weight, distribute\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input data to be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input data to be non-empty."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# 1. Texto base (corpus largo)\n",
    "# ============================\n",
    "text = (\n",
    "    \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, \"\n",
    "    \"no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, \"\n",
    "    \"adarga antigua, rocín flaco y galgo corredor. \"\n",
    "    \"Este texto es suficientemente largo para entrenar un modelo de prueba.\"\n",
    ")\n",
    "\n",
    "# Diccionario de caracteres\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Convertir texto a índices\n",
    "seq = [char2idx[c] for c in text]\n",
    "\n",
    "# ============================\n",
    "# 2. Preparar dataset\n",
    "# ============================\n",
    "seq_length = 40  # longitud de secuencia\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(seq)\n",
    "\n",
    "# Crear secuencias de longitud seq_length+1\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=False)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(100).batch(64, drop_remainder=True)\n",
    "\n",
    "# ============================\n",
    "# 3. Definir modelo\n",
    "# ============================\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 64\n",
    "rnn_units = 128\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "\n",
    "# ============================\n",
    "# 4. Entrenamiento\n",
    "# ============================\n",
    "history = model.fit(dataset, epochs=10)\n",
    "\n",
    "# ============================\n",
    "# 5. Función de generación\n",
    "# ============================\n",
    "def generate_text(model, start_string, num_generate=200):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "    temperature = 1.0  # controla aleatoriedad\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = predictions[:, -1, :] / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], axis=1)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# ============================\n",
    "# 6. Probar generación\n",
    "# ============================\n",
    "print(generate_text(model, start_string=\"En un lugar \"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
